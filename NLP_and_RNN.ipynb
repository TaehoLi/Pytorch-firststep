{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC6g3TMZ_MM6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "emb = nn.Embedding(10000, 20, padding_idx=0)\n",
    "# Embedding 계층의 입력은 int64 Tensor\n",
    "inp = torch.tensor([1, 2, 5, 2, 10], dtype=torch.int64)\n",
    "# 출력은 float32 Tensor\n",
    "out = emb(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19090,
     "status": "ok",
     "timestamp": 1544779845323,
     "user": {
      "displayName": "winston kim",
      "photoUrl": "",
      "userId": "05942964544969189760"
     },
     "user_tz": -480
    },
    "id": "X5ZhVl_4DUT2",
    "outputId": "768a7618-5c02-425b-b3df-1be239b111a6"
   },
   "outputs": [],
   "source": [
    "#!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6FflS90DlFV"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "remove_marks_regex = re.compile(\"[,\\.\\(\\)\\[\\]\\*:;]|<.*?>\")\n",
    "shift_marks_regex = re.compile(\"([?!])\")\n",
    "\n",
    "def text2ids(text, vocab_dict):\n",
    "    # !? 이외의 기호 삭제\n",
    "    text = remove_marks_regex.sub(\"\", text)\n",
    "    # !?와 단어 사이에 공백 삽입\n",
    "    text = shift_marks_regex.sub(r\" \\1 \", text)\n",
    "    tokens = text.split()\n",
    "    return [vocab_dict.get(token, 0) for token in tokens]\n",
    "\n",
    "def list2tensor(token_idxes, max_len=100, padding=True):\n",
    "    if len(token_idxes) > max_len:\n",
    "        token_idxes = token_idxes[:max_len]\n",
    "    n_tokens = len(token_idxes)\n",
    "    if padding:\n",
    "        token_idxes = token_idxes \\\n",
    "            + [0] * (max_len - len(token_idxes))\n",
    "    return torch.tensor(token_idxes, dtype=torch.int64), n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJnQl4foGnTk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import (Dataset, \n",
    "                              DataLoader,\n",
    "                              TensorDataset)\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXFhcgVrG0Am"
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dir_path, train=True,\n",
    "                 max_len=100, padding=True):\n",
    "        self.max_len = max_len\n",
    "        self.padding = padding\n",
    "        \n",
    "        path = pathlib.Path(dir_path)\n",
    "        vocab_path = path.joinpath(\"imdb.vocab\")\n",
    "        \n",
    "        # 용어집 파일을 읽어서 행 단위로 분할\n",
    "        self.vocab_array = vocab_path.open() \\\n",
    "                            .read().strip().splitlines()\n",
    "        # 단어가 키이고 값이 ID인 dict 만들기\n",
    "        self.vocab_dict = dict((w, i+1) \\\n",
    "            for (i, w) in enumerate(self.vocab_array))\n",
    "        \n",
    "        if train:\n",
    "            target_path = path.joinpath(\"train\")\n",
    "        else:\n",
    "            target_path = path.joinpath(\"test\")\n",
    "        pos_files = sorted(glob.glob(\n",
    "            str(target_path.joinpath(\"pos/*.txt\"))))\n",
    "        neg_files = sorted(glob.glob(\n",
    "            str(target_path.joinpath(\"neg/*.txt\"))))\n",
    "        # pos는 1, neg는 0인 label을 붙여서\n",
    "        # (file_path, label)의 튜플 리스트 작성\n",
    "        self.labeled_files = \\\n",
    "            list(zip([0]*len(neg_files), neg_files )) + \\\n",
    "            list(zip([1]*len(pos_files), pos_files))\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_array)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label, f = self.labeled_files[idx]\n",
    "        # 파일의 텍스트 데이터를 읽어서 소문자로 변환\n",
    "        data = open(f).read().lower()\n",
    "        # 텍스트 데이터를 ID 리스트로 변환\n",
    "        data = text2ids(data, self.vocab_dict)\n",
    "        # ID 리스트를 Tensor로 변환\n",
    "        data, n_tokens = list2tensor(data, self.max_len, self.padding)\n",
    "        return data, label, n_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gDAC2TtJRf3"
   },
   "outputs": [],
   "source": [
    "train_data = IMDBDataset(\"data/aclImdb/\")\n",
    "test_data = IMDBDataset(\"data/aclImdb/\", train=False)\n",
    "train_loader = DataLoader(train_data, batch_size=32,\n",
    "                          shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=32,\n",
    "                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMR3iVVyLS49"
   },
   "outputs": [],
   "source": [
    "class SequenceTaggingNet(nn.Module):\n",
    "    def __init__(self, num_embeddings,\n",
    "                 embedding_dim=50, \n",
    "                 hidden_size=50,\n",
    "                 num_layers=1,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n",
    "                            padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x, h0=None, l=None):\n",
    "        # ID를 Embedding으로 다차원 벡터로 변환\n",
    "        # x는 (batch_size, step_size) \n",
    "        # -> (batch_size, step_size, embedding_dim)\n",
    "        x = self.emb(x)\n",
    "        # 초기 상태 h0와 함께 RNN에 x를 전달\n",
    "        # x는(batch_size, step_size, embedding_dim)\n",
    "        # -> (batch_size, step_size, hidden_dim)\n",
    "        x, h = self.lstm(x, h0)\n",
    "        # 마지막 단계만 추출\n",
    "        # xは(batch_size, step_size, hidden_dim)\n",
    "        # -> (batch_size, 1)\n",
    "        if l is not None:\n",
    "            # 입력의 원래 길이가 있으면 그것을 이용\n",
    "            x = x[list(range(len(x))), l-1, :]\n",
    "        else:\n",
    "            # 없으면 단순히 마지막 것을 이용\n",
    "            x = x[:, -1, :]\n",
    "        # 추출한 마지막 단계를 선형 계층에 넣는다\n",
    "        x = self.linear(x)\n",
    "        # 불필요한 차원을 삭제\n",
    "        # (batch_size, 1) -> (batch_size, )\n",
    "        x = x.squeeze()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTNR-rYZSF5f"
   },
   "outputs": [],
   "source": [
    "def eval_net(net, data_loader, device=\"cpu\"):\n",
    "    net.eval()\n",
    "    ys = []\n",
    "    ypreds = []\n",
    "    for x, y, l in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        l = l.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = net(x, l=l)\n",
    "            y_pred = (y_pred > 0).long()\n",
    "            ys.append(y)\n",
    "            ypreds.append(y_pred)\n",
    "    ys = torch.cat(ys)\n",
    "    ypreds = torch.cat(ypreds)\n",
    "    acc = (ys == ypreds).float().sum() / len(ys)\n",
    "    return acc.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nl7MxgBwSNds"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:08<00:00, 96.79it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.674502241641969 0.5557599663734436 0.5483999848365784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:07<00:00, 97.87it/s] \n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6536601684282503 0.7119199633598328 0.6725599765777588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:08<00:00, 94.02it/s] \n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.5632505950415531 0.7936399579048157 0.7324000000953674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:07<00:00, 99.22it/s] \n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.44623549179652766 0.8387199640274048 0.756879985332489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:07<00:00, 97.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.36834771029860774 0.8714399933815002 0.7651199698448181\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# num_embeddings에는 0을 포함해서 train_data.vocab_size+1를 넣는다\n",
    "net = SequenceTaggingNet(train_data.vocab_size+1, \n",
    "num_layers=2)\n",
    "net.to(\"cuda:0\")\n",
    "opt = optim.Adam(net.parameters())\n",
    "loss_f = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    losses = []\n",
    "    net.train()\n",
    "    for x, y, l in tqdm.tqdm(train_loader):\n",
    "        x = x.to(\"cuda:0\")\n",
    "        y = y.to(\"cuda:0\")\n",
    "        l = l.to(\"cuda:0\")\n",
    "        y_pred = net(x, l=l)\n",
    "        loss = loss_f(y_pred, y.float())\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    train_acc = eval_net(net, train_loader, \"cuda:0\")\n",
    "    val_acc = eval_net(net, test_loader, \"cuda:0\")\n",
    "    print(epoch, mean(losses), train_acc, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85864,
     "status": "ok",
     "timestamp": 1544784319445,
     "user": {
      "displayName": "winston kim",
      "photoUrl": "",
      "userId": "05942964544969189760"
     },
     "user_tz": -480
    },
    "id": "EwBTsA03T2bi",
    "outputId": "b0149576-2eb7-4423-9a01-c627523deb0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taeho/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.93152, 0.394)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_X, train_y = load_svmlight_file(\n",
    "    \"data/aclImdb/train/labeledBow.feat\")\n",
    "test_X, test_y = load_svmlight_file(\n",
    "    \"data/aclImdb/test/labeledBow.feat\",\n",
    "    n_features=train_X.shape[1])\n",
    "\n",
    "model = LogisticRegression(C=0.1, max_iter=500, solver='lbfgs', multi_class='auto')\n",
    "model.fit(train_X, train_y)\n",
    "model.score(train_X, train_y), model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phV7ctdUWDmz"
   },
   "outputs": [],
   "source": [
    "class SequenceTaggingNet2(SequenceTaggingNet):\n",
    "    def forward(self, x, h0=None, l=None):\n",
    "        # ID를 Embedding으로 다차원 벡터로 변환\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # 길이가 주어진 경우 PckedSequence 만들기\n",
    "        if l is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, l, batch_first=True)\n",
    "        \n",
    "        # RNN에 입력\n",
    "        x, h = self.lstm(x, h0)\n",
    "        \n",
    "        # 마지막 단계를 추출해서 선형 계층에 넣는다\n",
    "        if l is not None:\n",
    "            # 길이 정보가 있으면 마지막 계층의\n",
    "            # 내부 상태 벡터를 직접 이용할 수 있다\n",
    "            # LSTM는 보통 내부 상태 외에 블럭 셀 상태도\n",
    "            # 가지고 있으므로 내부 상태만 사용한다\n",
    "            hidden_state, cell_state = h\n",
    "            x = hidden_state[-1]\n",
    "        else:\n",
    "            x = x[:, -1, :]\n",
    "        \n",
    "        # 선형 계층에 넣는다\n",
    "        x = self.linear(x).squeeze()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOku_ZtUXBuX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:09<00:00, 100.77it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.30828448857092644 0.9049199819564819 0.7806400060653687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:07<00:00, 99.31it/s] \n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.2572339070517846 0.9299599528312683 0.7843999862670898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:07<00:00, 99.12it/s] \n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.20926989355812903 0.936519980430603 0.7718799710273743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:07<00:00, 99.27it/s] \n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.16977799377973427 0.9572399854660034 0.7816799879074097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:08<00:00, 97.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.13514550855321347 0.9682799577713013 0.785319983959198\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    losses = []\n",
    "    net.train()\n",
    "    for x, y, l in tqdm.tqdm(train_loader):\n",
    "        # 길이 배열을 길이 순으로 정렬\n",
    "        l, sort_idx = torch.sort(l, descending=True)\n",
    "        # 얻은 인덱스를 사용해서 x,y도 정렬\n",
    "        x = x[sort_idx]\n",
    "        y = y[sort_idx]\n",
    "        \n",
    "        x = x.to(\"cuda:0\")\n",
    "        y = y.to(\"cuda:0\")\n",
    "        \n",
    "        y_pred = net(x, l=l)\n",
    "        loss = loss_f(y_pred, y.float())\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    train_acc = eval_net(net, train_loader, \"cuda:0\")\n",
    "    val_acc = eval_net(net, test_loader, \"cuda:0\")\n",
    "    print(epoch, mean(losses), train_acc, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f53dSAs0R0XG"
   },
   "outputs": [],
   "source": [
    "# 모든 ascii 문자로 사전 만들기\n",
    "import string\n",
    "all_chars = string.printable\n",
    "\n",
    "vocab_size = len(all_chars)\n",
    "vocab_dict = dict((c, i) for (i, c) in enumerate(all_chars))\n",
    "\n",
    "# 문자열을 수치 리스트로 변환하는 함수\n",
    "def str2ints(s, vocab_dict):\n",
    "    return [vocab_dict[c] for c in s]\n",
    "  \n",
    "# 수치 리스트를 문자열로 변환하는 함수\n",
    "def ints2str(x, vocab_array):\n",
    "    return \"\".join([vocab_array[i] for i in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qVk_tWpm3dk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import (Dataset, \n",
    "                           DataLoader,\n",
    "                           TensorDataset)\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQvNRnvGnGMX"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, path, chunk_size=200):\n",
    "        # 파일을 읽어서 수치 리스트로 변환\n",
    "        data = str2ints(open(path).read().strip(), vocab_dict)\n",
    "        \n",
    "        # Tensor로 변환해서 split 한다\n",
    "        data = torch.tensor(data, dtype=torch.int64).split(chunk_size)\n",
    "        \n",
    "        # 마지막 덩어리(chunk)의 길이를 확인해서 부족한 경우 버린다後のchunkの長さをチェックして足りない場合には捨てる\n",
    "        if len(data[-1]) < chunk_size:\n",
    "            data = data[:-1]\n",
    "        \n",
    "        self.data = data\n",
    "        self.n_chunks = len(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_chunks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oy81AcMxoeeR"
   },
   "outputs": [],
   "source": [
    "ds = ShakespeareDataset(\"data/tinyshakespeare.txt\",  chunk_size=200)\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDyTY8x4uRiX"
   },
   "outputs": [],
   "source": [
    "class SequenceGenerationNet(nn.Module):\n",
    "    def __init__(self, num_embeddings, \n",
    "                 embedding_dim=50, \n",
    "                 hidden_size=50,\n",
    "                 num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout)\n",
    "        # Linear의 output 크기는 첫 Embedding의 \n",
    "        # input 크기와 같은 num_embeddings\n",
    "        self.linear = nn.Linear(hidden_size, num_embeddings)\n",
    "        \n",
    "    def forward(self, x, h0=None):\n",
    "        x = self.emb(x)\n",
    "        x, h = self.lstm(x, h0)\n",
    "        x = self.linear(x)\n",
    "        return x, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQ4vfribu8i1"
   },
   "outputs": [],
   "source": [
    "def generate_seq(net, start_phrase=\"The King said \",\n",
    "                 length=200, temperature=0.8, device=\"cpu\"):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    net.eval()\n",
    "    # 출력 수치를 저장할 리스트\n",
    "    result = []\n",
    "    \n",
    "    # 시작 문자열을 Tensor로 변환\n",
    "    start_tensor = torch.tensor(\n",
    "        str2ints(start_phrase, vocab_dict),\n",
    "        dtype=torch.int64\n",
    "    ).to(device)\n",
    "    # 선두에 batch 차원을 붙인다\n",
    "    x0 = start_tensor.unsqueeze(0) \n",
    "    # RNN을 통해서 출력과 새로운 내부 상태를 얻는다\n",
    "    o, h = net(x0)\n",
    "    # 출력을 정규화돼있지 않은 확률로 변환\n",
    "    out_dist = o[:, -1].view(-1).exp()\n",
    "    # 확률로부터 실제 문자의 인덱스를 샘플링グ\n",
    "    top_i = torch.multinomial(out_dist, 1)[0]\n",
    "    # 결과 저장\n",
    "    result.append(top_i)\n",
    "    \n",
    "    # 생성된 결과를 차례로 RNN에 넣는다\n",
    "    for i in range(length):\n",
    "        inp = torch.tensor([[top_i]], dtype=torch.int64)\n",
    "        inp = inp.to(device)\n",
    "        o, h = net(inp, h)\n",
    "        out_dist = o.view(-1).exp()\n",
    "        top_i = torch.multinomial(out_dist, 1)[0]\n",
    "        result.append(top_i)\n",
    "    \n",
    "    # 시작 문자열과 생성된 문자열을 모아서 반환\n",
    "    return start_phrase + ints2str(result, all_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 453010,
     "status": "ok",
     "timestamp": 1545029221763,
     "user": {
      "displayName": "winston kim",
      "photoUrl": "",
      "userId": "05942964544969189760"
     },
     "user_tz": -480
    },
    "id": "GrrqpgRy4P-F",
    "outputId": "8680bcb6-91b6-44dd-e84e-8d567e23e8a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 79.48it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.470447996684483\n",
      "The King said om aoNteriEsseor ta wiy Vut\n",
      "i 7siotetde' o,du ue.oto  mnG -:efclnoyl td Af hyLuefmlu oona4 hoe i IittheYhNmee\n",
      " Cholsrs\n",
      "yr ooan s\n",
      "ei eirC glty hc cshuisUs  w yese bvlYga r ra\n",
      " ooP\n",
      "f orug e w I  ty oshwe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 76.19it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.8910851655687604\n",
      "The King said hdi.ce\n",
      "serl nonr camenas bsaciaclnce;ns melanWmhs mect catd ceosun mo\n",
      "Lhiyt o toter coLlcho;it, gi. b le.u\n",
      "\n",
      "OWLwIIOTI\n",
      "Sher Ieom fotn rthih?pib hl defas\n",
      "Seefes bode so nyib\n",
      "Ieler wo -atalo'uleb t'emd so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 79.06it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.517603942326137\n",
      "The King said ghem gagares-hl, hinfendin fepnev Mil, she's,\n",
      "\n",
      "Fown\n",
      "Mo shoriend I seio dors the: ouur, hadn farnayh gouv\n",
      "whuinle ma weannt ace'lib syout yinnethrore s pasd aw ticovic id Ih shotem Keut thans sthib that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 81.72it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.367422117505755\n",
      "The King said sham soafh hiln thir kice tho I heud hith tham if ins me, othen theele ted an\n",
      "At shafg'd fu he bou\n",
      " wauld.\n",
      "\n",
      "ROAEHS:\n",
      "Drid'd ad let dorened, pyease seat to mind I lof rhapy\n",
      "Whe.\n",
      "dewy:-nk,\n",
      "'nsam-mer the c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 82.94it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.2789749894823346\n",
      "The King said thisas sout ime mind me wot to thicitidinged\n",
      "Loust yeu wher miss: deitume in sameaven beclakitfy dyout ank.\n",
      "\n",
      "TGAONIN LENOI:\n",
      "Santeld\n",
      "Zof efot at doull hire? ywons wat,ly a pato pot arof mis'y bace lort \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 78.49it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.2064720439910888\n",
      "The King said through tuus omang.\n",
      "\n",
      "VINLARCICHULUNA:\n",
      "No; gcinkth thaut youlm.\n",
      "His deavesty wou;\n",
      "Hit swimiin offore thy nealg sore Ente and so sfy.\n",
      "\n",
      "INP, Athith in,\n",
      "fore twest me a famis no find suons,\n",
      "The macant, ili\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 81.92it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.1429469517299107\n",
      "The King said utey, be if beince oml?\n",
      "\n",
      "BLIUMS:\n",
      "Po file by shesturte the, mitrer thak ou be my hoth lens\n",
      "Muw iny luls nomdis parden gorp loter jot on rellina.\n",
      "\n",
      "Fyirg suy and soul kpie's\n",
      "Is satume at hof yabner full m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 81.93it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.0892248153686523\n",
      "The King said in pord these therer,\n",
      "Caroning wounq fomer.\n",
      "\n",
      "KHRRPOS:\n",
      "Had sty Loumse'd be\n",
      "the kucesTy co amise shesth;\n",
      "And my hay bisveess.\n",
      "\n",
      "CADDE PROKE:\n",
      "nove that poucied,\n",
      "Fek how he fon choutiung a-deref I \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 82.74it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.043974597794669\n",
      "The King said cott\n",
      "I reow thid, lrokedead thy muare and sand faist if;\n",
      "The hi< loted dourn falth: be Giriiemens:\n",
      "That heam dao-, is of her fiessy the bean! unglyercore maguar.\n",
      "\n",
      "BI MINGMA:\n",
      "Vey inkernors but felonemy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:02<00:00, 81.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.0063084397997173\n",
      "The King said for you yit;\n",
      "And kiver tut snow he on Ell? Or him I\n",
      "Why goodfrone:\n",
      "What?\n",
      "Hirned to owam the ore with omfens e'ut am think thigs;\n",
      "Wo eart thet sust of erper an a bovang.\n",
      "Af this plarly it to our pite La\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "net = SequenceGenerationNet(vocab_size, 20, 50,\n",
    "                            num_layers=2, dropout=0.1)\n",
    "net.to(\"cuda:0\")\n",
    "opt = optim.Adam(net.parameters())\n",
    "# 다중 식별 문제이므로 SoftmaxCrossEntropyLoss가 손실 함수가 된다\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    net.train()\n",
    "    losses = []\n",
    "    for data in tqdm.tqdm(loader):\n",
    "        # x는 처음부터 마지막의 하나 앞 문자까지\n",
    "        x = data[:, :-1]\n",
    "        # y는 두 번째부터 마지막 문자까지\n",
    "        y = data[:, 1:]\n",
    "        \n",
    "        x = x.to(\"cuda:0\")\n",
    "        y = y.to(\"cuda:0\")\n",
    "        \n",
    "        y_pred, _ = net(x)\n",
    "        # batch와 step 축을 통합해서 CrossEntropyLoss에 전달\n",
    "        loss = loss_f(y_pred.view(-1, vocab_size), y.view(-1))\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    # 현재 손실 함수와 생성된 문장 예 표시\n",
    "    print(epoch, mean(losses))\n",
    "    with torch.no_grad():\n",
    "        print(generate_seq(net, device=\"cuda:0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENIJww_tFGOZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-04 23:00:32--  http://www.manythings.org/anki/spa-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6cc4, ...\n",
      "접속 www.manythings.org (www.manythings.org)|104.24.108.196|:80... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2819791 (2.7M) [application/zip]\n",
      "Saving to: ‘spa-eng.zip’\n",
      "\n",
      "spa-eng.zip         100%[===================>]   2.69M  1.65MB/s    in 1.6s    \n",
      "\n",
      "2019-06-04 23:00:35 (1.65 MB/s) - ‘spa-eng.zip’ saved [2819791/2819791]\n",
      "\n",
      "Archive:  spa-eng.zip\n",
      "  inflating: _about.txt              \n",
      "  inflating: spa.txt                 \n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/spa-eng.zip\n",
    "!unzip spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmNuXZL_Ff6-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import (Dataset, \n",
    "                              DataLoader,\n",
    "                              TensorDataset)\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHXbSSd5FRdi"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "remove_marks_regex = re.compile(\n",
    "    \"[\\,\\(\\)\\[\\]\\*:;¿¡]|<.*?>\")\n",
    "shift_marks_regex = re.compile(\"([?!\\.])\")\n",
    "\n",
    "unk = 0\n",
    "sos = 1\n",
    "eos = 2\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    # 불필요한 문자 제거\n",
    "    text = remove_marks_regex.sub(\"\", text)\n",
    "    # ?!. 와 단어 사이에 공백 삽입\n",
    "    text = shift_marks_regex.sub(r\" \\1\", text)\n",
    "    return text\n",
    "  \n",
    "def parse_line(line):\n",
    "    line = normalize(line.strip())\n",
    "    # 번역 대상(src)과 번역 결과(trg) 각각의 토큰을 리스트로 만든다\n",
    "    src, trg = line.split(\"\\t\")\n",
    "    src_tokens = src.strip().split()\n",
    "    trg_tokens = trg.strip().split()\n",
    "    return src_tokens, trg_tokens\n",
    "  \n",
    "def build_vocab(tokens):\n",
    "    # 파일 안의 모든 문장에서 토큰의 등장 횟수를 확인\n",
    "    counts = collections.Counter(tokens)\n",
    "    # 토큰의 등장 횟수를 많은 순으로 나열\n",
    "    sorted_counts = sorted(counts.items(), \n",
    "                           key=lambda c: c[1], reverse=True)\n",
    "    # 세 개의 태그를 추가해서 정방향 리스트와 역방향 용어집 만들기\n",
    "    word_list = [\"<UNK>\", \"<SOS>\", \"<EOS>\"] \\\n",
    "        + [x[0] for x in sorted_counts]\n",
    "    word_dict = dict((w, i) for i, w in enumerate(word_list))\n",
    "    return word_list, word_dict\n",
    "  \n",
    "def words2tensor(words, word_dict, max_len, padding=0):\n",
    "    # 끝에 종료 태그를 붙임\n",
    "    words = words + [\"<EOS>\"]\n",
    "    # 사전을 이용해서 수치 리스트로 변환\n",
    "    words = [word_dict.get(w, 0) for w in words]\n",
    "    seq_len = len(words)\n",
    "    # 길이가 max_len이하이면 패딩한다\n",
    "    if seq_len < max_len + 1:\n",
    "        words = words + [padding] * (max_len + 1 - seq_len)\n",
    "    # Tensor로 변환해서 반환\n",
    "    return torch.tensor(words, dtype=torch.int64), seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAHMfV8Qmw_5"
   },
   "outputs": [],
   "source": [
    "class TranslationPairDataset(Dataset):\n",
    "    def __init__(self, path, max_len=15):\n",
    "        # 단어 수사 많은 문장을 걸러내는 함수\n",
    "        def filter_pair(p):\n",
    "            return not (len(p[0]) > max_len \n",
    "                        or len(p[1]) > max_len)\n",
    "        # 파일을 열어서, 파스 및 필터링       \n",
    "        with open(path) as fp:\n",
    "            pairs = map(parse_line, fp)\n",
    "            pairs = filter(filter_pair, pairs)\n",
    "            pairs = list(pairs)\n",
    "        # 문장의 소스와 타켓으로 나눔\n",
    "        src = [p[0] for p in pairs]\n",
    "        trg = [p[1] for p in pairs]\n",
    "        #각각의 어휘집 작성\n",
    "        self.src_word_list, self.src_word_dict = \\\n",
    "            build_vocab(itertools.chain.from_iterable(src))\n",
    "        self.trg_word_list, self.trg_word_dict = \\\n",
    "            build_vocab(itertools.chain.from_iterable(trg))\n",
    "        # 어휘집을 사용해서 Tensor로 변환\n",
    "        self.src_data = [words2tensor(\n",
    "            words, self.src_word_dict, max_len)\n",
    "                for words in src]\n",
    "        self.trg_data = [words2tensor(\n",
    "            words, self.trg_word_dict, max_len, -100)\n",
    "                         for words in trg]\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        src, lsrc = self.src_data[idx]\n",
    "        trg, ltrg = self.trg_data[idx]\n",
    "        return src, lsrc, trg, ltrg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gge2cxkUnwVd"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = 10\n",
    "path = \"data/spa.txt\"\n",
    "ds = TranslationPairDataset(path, max_len=max_len)\n",
    "loader = DataLoader(ds, batch_size=batch_size, shuffle=True,\n",
    "                    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oh7y_8tgoTlm"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_embeddings,\n",
    "                 embedding_dim=50, \n",
    "                  hidden_size=50,\n",
    "                 num_layers=1,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, \n",
    "          embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_size, num_layers,\n",
    "                            batch_first=True,\n",
    "dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, h0=None, l=None):\n",
    "        x = self.emb(x)\n",
    "        if l is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, l, batch_first=True)\n",
    "        _, h = self.lstm(x, h0)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHC35oMSoVyD"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_embeddings,\n",
    "                 embedding_dim=50, \n",
    "                 hidden_size=50,\n",
    "                 num_layers=1,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size,\n",
    "                            num_layers, batch_first=True,\n",
    "                            dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, num_embeddings)\n",
    "    \n",
    "    def forward(self, x, h, l=None):\n",
    "        x = self.emb(x)\n",
    "        if l is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, l, batch_first=True)\n",
    "        x, h = self.lstm(x, h)\n",
    "        if l is not None:\n",
    "            x = nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0)[0]\n",
    "        x = self.linear(x)\n",
    "        return x, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCT4wbK-5DQ_"
   },
   "outputs": [],
   "source": [
    "def translate(input_str, enc, dec, max_len=15, device=\"cpu\"):\n",
    "    # 입력 문자열을 수치화해서 Tensor로 변환\n",
    "    words = normalize(input_str).split()\n",
    "    input_tensor, seq_len = words2tensor(words, \n",
    "        ds.src_word_dict, max_len=max_len)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "    # 엔코더에서 사용하므로 입력값의 길이도 리스트로 만들어둔다\n",
    "    seq_len = [seq_len]\n",
    "    # 시작 토큰 준비\n",
    "    sos_inputs = torch.tensor(sos, dtype=torch.int64)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    sos_inputs = sos_inputs.to(device)\n",
    "    # 입력 문자열을 엔코더에 넣어서 컨텍스트 얻기\n",
    "    ctx = enc(input_tensor, l=seq_len)\n",
    "    # 시작 토큰과 컨텍스트를 디코더의 초깃값으로 설정\n",
    "    z = sos_inputs\n",
    "    h = ctx\n",
    "    results = []\n",
    "    for i in range(max_len):\n",
    "        # Decoder로 다음 단어 예측\n",
    "        o, h = dec(z.view(1, 1), h)\n",
    "        # 선형 계층의 출력이 가장 큰 위치가 다음 단어의 ID\n",
    "        wi = o.detach().view(-1).max(0)[1]\n",
    "        if wi.item() == eos:\n",
    "            break\n",
    "        results.append(wi.item())\n",
    "        # 다음 입력값으로 현재 출력 ID를 사용\n",
    "        z = wi\n",
    "    # 기록해둔 출력 ID를 문자열로 변환\n",
    "    return \" \".join(ds.trg_word_list[i] for i in results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1545045864966,
     "user": {
      "displayName": "winston kim",
      "photoUrl": "",
      "userId": "05942964544969189760"
     },
     "user_tz": -480
    },
    "id": "tftEQdHD6OVH",
    "outputId": "25ff72c2-90b5-4231-dea7-bb636dead0e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mareé indica comenzara \"gracias \"gracias prendé \"gracias \"gracias \"gracias \"gracias \"gracias \"gracias \"gracias \"gracias \"gracias'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(len(ds.src_word_list), 100, 100, 2)\n",
    "dec = Decoder(len(ds.trg_word_list), 100, 100, 2)\n",
    "translate(\"I am a student.\", enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKFqA3t9608o"
   },
   "outputs": [],
   "source": [
    "enc = Encoder(len(ds.src_word_list), 100, 100, 2)\n",
    "dec = Decoder(len(ds.trg_word_list), 100, 100, 2)\n",
    "enc.to(\"cuda:0\")\n",
    "dec.to(\"cuda:0\")\n",
    "opt_enc = optim.Adam(enc.parameters(), 0.002)\n",
    "opt_dec = optim.Adam(dec.parameters(), 0.002)\n",
    "loss_f = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1835
    },
    "colab_type": "code",
    "id": "XcWJo1rn7X2r",
    "outputId": "d481dd6a-f753-455a-a912-4fe6887c4c2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:48<00:00, 33.15it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.401043785844992\n",
      "es un problema .\n",
      "no le gusta la escuela .\n",
      "es mi padre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:34<00:00, 46.54it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.441772671591993\n",
      "es un problema .\n",
      "no le gusta la cena .\n",
      "es mi nombre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:33<00:00, 48.33it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.2154853361591615\n",
      "es un estudiante .\n",
      "me gusta comer .\n",
      "es mi madre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:34<00:00, 47.49it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.6978825399042718\n",
      "soy un estudiante .\n",
      "me gusta viajar .\n",
      "es mi madre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:33<00:00, 47.96it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1.4522621694789282\n",
      "es un estudiante .\n",
      "no le gusta comer pescado .\n",
      "es mi madre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:33<00:00, 47.81it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1.2824439269778027\n",
      "es un estudiante .\n",
      "no le gusta comer pizza .\n",
      "es mi madre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:41<00:00, 39.33it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1.152553113463185\n",
      "es un estudiante .\n",
      "no le gusta comer pizza .\n",
      "es mi madre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:48<00:00, 33.31it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1.0492621200068528\n",
      "es un estudiante .\n",
      "él quiere pizza .\n",
      "es mi madre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:48<00:00, 33.30it/s]\n",
      "  0%|          | 0/1623 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.964308579065002\n",
      "es un estudiante .\n",
      "no le gusta comer pizza .\n",
      "es mi madre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:48<00:00, 33.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.897372405946218\n",
      "es un estudiante .\n",
      "él quiere pizza .\n",
      "es mi madre .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def to2D(x):\n",
    "    shapes = x.shape\n",
    "    return x.reshape(shapes[0] * shapes[1], -1)\n",
    "  \n",
    "for epoc in range(10):\n",
    "    # 신경망을 훈련 모드로 설정\n",
    "    enc.train(), dec.train()\n",
    "    losses = []\n",
    "    for x, lx, y, ly in tqdm.tqdm(loader):\n",
    "        # x의 PackedSequence를 만들기 위해 번역 소스의 길이로 내림차순 정렬한다\n",
    "        lx, sort_idx = lx.sort(descending=True)\n",
    "        x, y, ly = x[sort_idx], y[sort_idx], ly[sort_idx]\n",
    "        x, y = x.to(\"cuda:0\"), y.to(\"cuda:0\")\n",
    "        # 번역 소스를 엔코더에 넣어서 컨텍스트를 얻는다\n",
    "        ctx = enc(x, l=lx)\n",
    "        # y의 PackedSequence를 만들기 위해 번역 소스의 길이로 내림차순 정렬\n",
    "        ly, sort_idx = ly.sort(descending=True)\n",
    "        y = y[sort_idx]\n",
    "        # Decoder의 초깃값 설정\n",
    "        h0 = (ctx[0][:, sort_idx, :], ctx[1][:, sort_idx, :])\n",
    "        z = y[:, :-1].detach()\n",
    "        # -100인 상태에선 Embedding 계산에서 오류가 발생하므로 0으로 변경\n",
    "        z[z==-100] = 0\n",
    "        # 디코더에 넣어서 손실 함수 계산\n",
    "        o, _ = dec(z, h0, l=ly-1)\n",
    "        loss = loss_f(to2D(o[:]), to2D(y[:, 1:max(ly)]).squeeze())\n",
    "        # Backpropagation(오차 역전파 실행)\n",
    "        enc.zero_grad(), dec.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_enc.step(), opt_dec.step()\n",
    "        losses.append(loss.item())\n",
    "    # 전체 데이터의 계산이 끝나면 현재의\n",
    "    # 손실 함수 값이나 번역 결과를 표시\n",
    "    enc.eval(), dec.eval()\n",
    "    print(epoc, mean(losses))\n",
    "    with torch.no_grad():\n",
    "        print(translate(\"I am a student.\",\n",
    "                         enc, dec, max_len=max_len, \n",
    "device=\"cuda:0\"))\n",
    "        print(translate(\"He likes to eat pizza.\",\n",
    "                         enc, dec, max_len=max_len, \n",
    "device=\"cuda:0\"))\n",
    "        print(translate(\"She is my mother.\",\n",
    "                         enc, dec, max_len=max_len, \n",
    "device=\"cuda:0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_chapter5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
